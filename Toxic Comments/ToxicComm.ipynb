{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8xWcfCO4M1g"
   },
   "source": [
    "## Описание проекта\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Постройте модель со значением метрики качества F1 не меньше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7ZC_4fzFyUV"
   },
   "source": [
    "## Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRYYk7oyDjYv",
    "outputId": "f952b0e3-6c2a-4e32-f714-4a6d34d75346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sergf\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\sergf\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from lightgbm) (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sergf\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install notifiers --quiet\n",
    "!pip install nltk\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UIHwixW4J96",
    "outputId": "eecf3e99-26c3-4378-e4d1-dc3e5b1ab6b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sergf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sergf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sergf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import re\n",
    "from notifiers import get_notifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ELa2gEJaDstu"
   },
   "outputs": [],
   "source": [
    "def make_notifier(token='1724537409:AAHuJxCLgdoMpP8c3StM4_sZj05DZ91T-BI',\n",
    "                 chat_id=-521030020):\n",
    "    def send(text):\n",
    "        telegram=get_notifier('telegram')\n",
    "        telegram.notify(message=text,\n",
    "                       token=token,\n",
    "                       chat_id=chat_id)\n",
    "    return send\n",
    "send = make_notifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k48tTku6F2N9"
   },
   "source": [
    "## Загружаем массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5uIFTzVN4ZpH"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "F08R-zTB4jVP",
    "outputId": "fdab0cfb-53d9-43d2-cd63-64d9f2fddf27"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giEb9W977Dja",
    "outputId": "1da09937-3ae3-4595-bdb2-6dd4a7300417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: таблица имеет 2 колонки и 159571 строки, пропусков нет, токсичных комментариев примерно 10% от общего массива."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTHVPRDFF7w6"
   },
   "source": [
    "## Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "G9KwoaKZgwLX"
   },
   "outputs": [],
   "source": [
    "nlp = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функции обработки комментариев: будем удалять знаки препинания, символы переноса строки, после этого лемматизируем слова и соединим обратно в предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UiH7TA5F_DcV"
   },
   "outputs": [],
   "source": [
    "def clear_text(row):\n",
    "    txt = re.sub(r'[^a-zA-Z ]', \" \", row)\n",
    "    return \" \".join(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ouKxV6Tc7Erj"
   },
   "outputs": [],
   "source": [
    "def lemm_text(row):\n",
    "    row = clear_text(row)\n",
    "    word_token = nltk.word_tokenize(row)\n",
    "    return \" \".join([nlp.lemmatize(word) for word in word_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Xw3fffaO7Eo8"
   },
   "outputs": [],
   "source": [
    "df['lem_text'] = df['text'].apply(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "id": "VwMvTGkN7EmV",
    "outputId": "9d5b1c96-1921-42ce-bbbf-e360153c1d0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lem_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He match this background colour I m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Congratulations from me a well use the tool we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Sorry if the word nonsense wa offensive to you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Fair use rationale for Image Wonju jpg Thanks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bbq \\n\\nbe a man and lets discuss it-maybe ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>bbq be a man and let discus it maybe over the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey what is it talk What is it an exclusive gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Before you start throwing accusations and warn...</td>\n",
       "      <td>0</td>\n",
       "      <td>Before you start throwing accusation and warni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oh, and the girl above started her arguments w...</td>\n",
       "      <td>0</td>\n",
       "      <td>Oh and the girl above started her argument wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...</td>\n",
       "      <td>0</td>\n",
       "      <td>Juelz Santanas Age In Juelz Santana wa year ol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Bye Don t look come or think of comming back T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski</td>\n",
       "      <td>0</td>\n",
       "      <td>REDIRECT Talk Voydan Pop Georgiev Chernodrinski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Mitsurugi point made no sense - why not ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>The Mitsurugi point made no sense why not argu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Don't mean to bother you \\n\\nI see that you're...</td>\n",
       "      <td>0</td>\n",
       "      <td>Don t mean to bother you I see that you re wri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  toxic  \\\n",
       "0   Explanation\\nWhy the edits made under my usern...      0   \n",
       "1   D'aww! He matches this background colour I'm s...      0   \n",
       "2   Hey man, I'm really not trying to edit war. It...      0   \n",
       "3   \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4   You, sir, are my hero. Any chance you remember...      0   \n",
       "5   \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7   Your vandalism to the Matt Shirvington article...      0   \n",
       "8   Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9   alignment on this subject and which are contra...      0   \n",
       "10  \"\\nFair use rationale for Image:Wonju.jpg\\n\\nT...      0   \n",
       "11  bbq \\n\\nbe a man and lets discuss it-maybe ove...      0   \n",
       "12  Hey... what is it..\\n@ | talk .\\nWhat is it......      1   \n",
       "13  Before you start throwing accusations and warn...      0   \n",
       "14  Oh, and the girl above started her arguments w...      0   \n",
       "15  \"\\n\\nJuelz Santanas Age\\n\\nIn 2002, Juelz Sant...      0   \n",
       "16  Bye! \\n\\nDon't look, come or think of comming ...      1   \n",
       "17   REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski      0   \n",
       "18  The Mitsurugi point made no sense - why not ar...      0   \n",
       "19  Don't mean to bother you \\n\\nI see that you're...      0   \n",
       "\n",
       "                                             lem_text  \n",
       "0   Explanation Why the edits made under my userna...  \n",
       "1   D aww He match this background colour I m seem...  \n",
       "2   Hey man I m really not trying to edit war It s...  \n",
       "3   More I can t make any real suggestion on impro...  \n",
       "4   You sir are my hero Any chance you remember wh...  \n",
       "5   Congratulations from me a well use the tool we...  \n",
       "6        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK  \n",
       "7   Your vandalism to the Matt Shirvington article...  \n",
       "8   Sorry if the word nonsense wa offensive to you...  \n",
       "9   alignment on this subject and which are contra...  \n",
       "10  Fair use rationale for Image Wonju jpg Thanks ...  \n",
       "11  bbq be a man and let discus it maybe over the ...  \n",
       "12  Hey what is it talk What is it an exclusive gr...  \n",
       "13  Before you start throwing accusation and warni...  \n",
       "14  Oh and the girl above started her argument wit...  \n",
       "15  Juelz Santanas Age In Juelz Santana wa year ol...  \n",
       "16  Bye Don t look come or think of comming back T...  \n",
       "17    REDIRECT Talk Voydan Pop Georgiev Chernodrinski  \n",
       "18  The Mitsurugi point made no sense why not argu...  \n",
       "19  Don t mean to bother you I see that you re wri...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yRLt4HCGCx2"
   },
   "source": [
    "## Базовая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим модель на обучающую и тестовую выборки с сохранением соотношения токсичных и нетоксичных комментариев в выборках, применим TF-IDF векторизацию к обучающей и тестовой выборке по отдельности, чтобы оценить важность слов по отношению к массиву"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FZURhZojzmPH"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['lem_text'], df['toxic'], test_size = 0.2, stratify = df['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = X_train.values.astype(\"U\")\n",
    "corpus_test = X_test.values.astype(\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-akFtMN7h92E",
    "outputId": "a595c9b1-15c5-49cd-adcd-467b3f1c5afa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words={'a', 'about', 'above', 'after', 'again', 'against',\n",
       "                            'ain', 'all', 'am', 'an', 'and', 'any', 'are',\n",
       "                            'aren', \"aren't\", 'as', 'at', 'be', 'because',\n",
       "                            'been', 'before', 'being', 'below', 'between',\n",
       "                            'both', 'but', 'by', 'can', 'couldn', \"couldn't\", ...})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tfidf = TfidfVectorizer(stop_words=stopwords)\n",
    "count_tfidf.fit(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW7mM-Ei7Ejo",
    "outputId": "18b1cd7d-4683-4eca-dc28-5700465a137b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 144060)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new = count_tfidf.transform(corpus_train)\n",
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель Логистической регрессии на полученных данных для получения первичной оценки метрики F1, как референсной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j50X0oHVh9lJ"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X_train_new, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t10fC1aPh9oe"
   },
   "outputs": [],
   "source": [
    "X_test_new = count_tfidf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ma_8ektjh9q9"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PsdN8N0fh9t7",
    "outputId": "680c4a72-e49b-45f9-fe50-b44b7856a51f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333956969130028"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: базовая модель дала результат F1 0.723, что ссужественно ниже, чем заявленный порог в 0.75, необходимо рассмотреть другие модели и подобрать гиперпараметры для них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxcKXTgMGGG4"
   },
   "source": [
    "## Подбор моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим таблицу-лог наших результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q0rMfvl6PYRO"
   },
   "outputs": [],
   "source": [
    "column = ['Модель', 'Параметры', 'Метрика F1']\n",
    "results = pd.DataFrame(columns = column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rB4I7LlCh9we"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    LGBMClassifier(),\n",
    "    LinearSVC()\n",
    "         ]\n",
    "\n",
    "# Подбор гиперпараметров для моделей ниже занял часов 6, поэтому я оставил только лучшие показатели, \n",
    "# но изменить список значений на range(start, stop, step) не составит сложности в случае необходимости.\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'model__penalty':['l1'],\n",
    "        'model__C':[1],\n",
    "        'model__solver':['liblinear'],\n",
    "        'model__random_state':[0]\n",
    "        },\n",
    "    {\n",
    "        'model__max_depth':[19],\n",
    "        'model__learning_rate' : [0.5],\n",
    "        'model__n_estimators':[170],\n",
    "        'model__random_state':[0]\n",
    "        },\n",
    "    {\n",
    "        'model__penalty':['l2'],\n",
    "        'model__C':[0.5],\n",
    "        'model__loss':['squared_hinge'],\n",
    "        'model__random_state':[0],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXPDjubmh9zP",
    "outputId": "e3d74201-26c4-410f-e203-142ddb0410b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    }
   ],
   "source": [
    "for model, param in zip(models, params):\n",
    "    pipe = Pipeline(steps = [('ti_idf', TfidfVectorizer(stop_words = set(nltk_stopwords.words('english')))),\n",
    "                   ('model', model)])\n",
    "    grid = GridSearchCV(estimator = pipe, param_grid = param, n_jobs =-1, scoring = 'f1', verbose = 4).fit(corpus_train, y_train)\n",
    "    f1_scor = f1_score(y_test, grid.best_estimator_.predict(corpus_test))\n",
    "    results = pd.concat([results, pd.DataFrame([[model, grid.best_params_, f1_scor]], columns = column)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Параметры</th>\n",
       "      <th>Метрика F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>{'model__C': 1, 'model__penalty': 'l1', 'model...</td>\n",
       "      <td>0.771110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier()</td>\n",
       "      <td>{'model__learning_rate': 0.5, 'model__max_dept...</td>\n",
       "      <td>0.768763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>{'model__C': 0.5, 'model__loss': 'squared_hing...</td>\n",
       "      <td>0.775947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Модель                                          Параметры  \\\n",
       "0  LogisticRegression()  {'model__C': 1, 'model__penalty': 'l1', 'model...   \n",
       "0      LGBMClassifier()  {'model__learning_rate': 0.5, 'model__max_dept...   \n",
       "0           LinearSVC()  {'model__C': 0.5, 'model__loss': 'squared_hing...   \n",
       "\n",
       "   Метрика F1  \n",
       "0    0.771110  \n",
       "0    0.768763  \n",
       "0    0.775947  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LZEHMGLoSWSC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>Параметры</th>\n",
       "      <th>Метрика F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC()</td>\n",
       "      <td>{'model__C': 0.5, 'model__loss': 'squared_hing...</td>\n",
       "      <td>0.775947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Модель                                          Параметры  Метрика F1\n",
       "0  LinearSVC()  {'model__C': 0.5, 'model__loss': 'squared_hing...    0.775947"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['Метрика F1']==results['Метрика F1'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "lyXGv8K17Egz"
   },
   "outputs": [],
   "source": [
    "send(f\"Код выполнен, лучший результат: {results[results['Метрика F1']==results['Метрика F1'].max()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: мы провели анализ массива на токсичные комментарии пользователей, обучили модели предсказывать такие комментарии, использовали метрику гармонического среднего - F1, лучший результат дала модель опорных векторов LinearSVC, метрика F1 составила 0.776"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Wikishop.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
